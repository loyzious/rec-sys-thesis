@InProceedings{Leavy2018,
  author          = {Susan Leavy},
  title           = {Gender Bias in Artificial Intelligence: The Need for Diversity and Gender Theory in Machine Learning},
  year            = {2018},
  address         = {Gothenburg, Sweden},
  pages           = {14--16},
  publisher       = {IEEE},
  abstract        = {Artificial intelligence is increasingly influencing the opinions and behaviour of people in everyday life. However, the over-representation of men in the design of these technologies could quietly undo decades of advances in gender equality. Over centuries, humans developed critical theory to inform decisions and avoid basing them solely on personal experience. However, machine intelligence learns primarily from observing data that it is presented with. While a machine's ability to process large volumes of data may address this in part, if that data is laden with stereotypical concepts of gender, the resulting application of the technology will perpetuate this bias. While some recent studies sought to remove bias from learned algorithms they largely ignore decades of research on how gender ideology is embedded in language. Awareness of this re-search and incorporating it into approaches to machine learning from text would help prevent the generation of biased algorithms. Leading thinkers in the emerging field addressing bias in artificial intelligence are also primarily female, suggesting that those who are potentially affected by bias are more likely to see, understand and attempt to resolve it. Gender balance in machine learning is therefore crucial to prevent algorithms from perpetuating gender ideologies that disadvantage women.},
  comment         = {Detect bias in ML alogrithm

ML system learn from biased data -> more men than women},
  date            = {27 May-3 June 2018},
  eventdate       = {27 May-3 June 2018},
  eventtitleaddon = {Gothenburg, Sweden},
  file            = {:pdfs/Gender_Bias_in_Artificial_Intelligence_The_Need_for_Diversity_and_Gender_Theory_in_Machine_Learning.pdf:PDF},
  groups          = {Gender Bias},
  isbn            = {978-1-5386-6187-1},
  journal         = {2018 IEEE/ACM 1st International Workshop on Gender Equality in Software Engineering (GE)},
  keywords        = {Machine learning algorithms, Machine learning, Training data, Linguistics, Media, Conferences, Gender Bias, Machine Learning, Text Analytics},
  ranking         = {rank4},
}

@InProceedings{Krueger2019,
  author          = {Stefan Krüger and Ben Hermann},
  title           = {Can an Online Service Predict Gender? On the State-of-the-Art in Gender Identification from Texts},
  year            = {2019},
  address         = {Montreal, QC, Canada},
  pages           = {13--16},
  publisher       = {IEEE},
  abstract        = {Gender equality initiatives are often faced with a problem: In order to determine whether initiatives are successful the gender of individuals in the target group must be known. As self-identification inherently has the problems that individuals have to respond and results may, therefore, be biased and incomplete, the temptation to use automated gender identification methods is evident. In the scientific literature, multiple sources ranging from the individual's name, their social media choices, biological features (e.g., brain scans or fingerprints), to texts attributed to the individual are used for automated gender identification with varying success. In this paper, we systematically inspect scientific publications for gender prediction based on textual data which are published between January 2017 and January 2019 in order to determine if such approaches may supply viable means to reliably determine an author's gender. However, we find that the best approach in the current state-of-the-art works with an accuracy of only 93.4%. Moreover, we discuss the possible harm that gender identification systems might entail due to their inaccuracy and also given that they are assuming a binary gender model. We conclude that gender identification based on textual data is currently no reliable substitute for self-identification.},
  comment         = {Is a literature review of  State-of-the-Art in Gender Identification from Texts},
  date            = {27-27 May 2019},
  doi             = {10.1109/GE.2019.00012},
  eventdate       = {27-27 May 2019},
  eventtitleaddon = {Montreal, QC, Canada},
  file            = {:pdfs/Can_an_Online_Service_Predict_Gender_On_the_State-of-the-Art_in_Gender_Identification_from_Texts.pdf:PDF},
  groups          = {Text Classification},
  isbn            = {978-1-7281-2246-5},
  journal         = {2019 IEEE/ACM 2nd International Workshop on Gender Equality in Software Engineering (GE)},
  keywords        = {Machine learning, Conferences, Gender issues, Databases, Tools, Facebook, Companies, gender equality, gender detection, gender identification},
  ranking         = {rank4},
}

@Article{Bernagozzi2021,
  author    = {Mariana Bernagozzi and Biplav Srivastava and Francesca Rossi and Sheema Usmani},
  journal   = {IEEE Internet Computing},
  title     = {Gender Bias in Online Language Translators: Visualization, Human Perception, and Bias/Accuracy Tradeoffs},
  year      = {2021},
  issn      = {1941-0131},
  pages     = {53--63},
  volume    = {25},
  abstract  = {Artificial intelligence(AI) systems that interact with humans, such as chatbots and language translators, have many useful applications. However, care must be put in addressing some concerns, such as the presence of bias, possible abusive language, and information leakage, that could hamper public trust in them. In this article, we focus on gender bias in online translators, as recognized and rated by a third-party assessment who does not have access to the training data, and we propose a visualization approach for such rating. We then conduct a survey of how users perceive bias in translators, whether they appreciate the proposed bias rating visualization, and how they may use it to reason about bias-accuracy trade-offs.},
  comment   = {Gender bias in ML systems},
  date      = {1 Sept.-Oct. 2021},
  doi       = {10.1109/MIC.2021.3097604},
  file      = {:pdfs/Gender_Bias_in_Online_Language_Translators_Visualization_Human_Perception_and_Bias_Accuracy_Tradeoffs.pdf:PDF},
  groups    = {Gender Bias},
  issue     = {5},
  keywords  = {Artificial intelligence, Internet, Data visualization, Visualization, Testing, Human computer interaction, Gender bias, AI services, Machine translation, Rating, User Survey},
  publisher = {IEEE},
  ranking   = {rank1},
}

@InProceedings{Krishnan2020,
  author          = {Anoop Krishnan and Ali Almadan and Ajita Rattani},
  title           = {Understanding Fairness of Gender Classification Algorithms Across Gender-Race Groups},
  year            = {2020},
  address         = {Miami, FL, USA},
  pages           = {1028--1035},
  publisher       = {IEEE},
  abstract        = {Automated gender classification has important applications in many domains, such as demographic research, law enforcement, online advertising, as well as human-computer interaction. Recent research has questioned the fairness of this technology across gender and race. Specifically, the majority of the studies raised the concern of higher error rates of the face-based gender classification system for darker-skinned people like African-American and for women. However, to date, the majority of existing studies were limited to African-American and Caucasian only. The aim of this paper is to investigate the differential performance of the gender classification algorithms across gender-race groups. To this aim, we investigate the impact of (a) architectural differences in the deep learning algorithms and (b) training set imbalance, as a potential source of bias causing differential performance across gender and race. Experimental investigations are conducted on two latest large-scale publicly available facial attribute datasets, namely, UTKFace and FairFace. The experimental results suggested that the algorithms with architectural differences varied in performance with consistency towards specific gender-race groups. For instance, for all the algorithms used, Black females (Black race in general) always obtained the least accuracy rates. Middle Eastern males and Latino females obtained higher accuracy rates most of the time. Training set imbalance further widens the gap in the unequal accuracy rates across all gender-race groups. Further investigations using facial landmarks suggested that facial morphological differences due to the bone structure influenced by genetic and environmental factors could be the cause of the least performance of Black females and Black race, in general.},
  comment         = {Image classification for gender-race bias


Gender classiﬁcation algorithms across gender-race groups},
  date            = {14-17 Dec. 2020},
  doi             = {10.1109/ICMLA51294.2020.00167},
  eventdate       = {14-17 Dec. 2020},
  eventtitleaddon = {Miami, FL, USA},
  file            = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Understanding_Fairness_of_Gender_Classification_Algorithms_Across_Gender-Race_Groups.pdf:PDF},
  groups          = {Image Classification},
  isbn            = {978-1-7281-8471-5},
  journal         = {2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  keywords        = {Training, Human computer interaction, Machine learning algorithms, Biometrics (access control), Law enforcement, Genetics, Classification algorithms, Fairness and Bias in AI, Facial Analysis, Gender Classification, Soft Biometrics, Usability and Human Interaction},
  ranking         = {rank4},
}

@InProceedings{Atay2021,
  author          = {Mustafa Atay and Hailey Gipson and Tony Gwyn and Kaushik Roy},
  title           = {Evaluation of Gender Bias in Facial Recognition with Traditional Machine Learning Algorithms},
  year            = {2021},
  address         = {Orlando, FL, USA},
  pages           = {1--7},
  publisher       = {IEEE},
  abstract        = {The prevalent commercial deployment of automated facial analysis systems such as face recognition as a robust authentication method has increasingly fueled scientific attention. Current machine learning algorithms allow for a relatively reliable detection, recognition, and categorization of face images comprised of age, race, and gender. Algorithms with such biased data are bound to produce skewed results. It leads to a significant decrease in the performance of state-of-the-art models when applied to images of gender or ethnicity groups. In this paper, we study the gender bias in facial recognition with gender balanced and imbalanced training sets using five traditional machine learning algorithms. We aim to report the machine learning classifiers which are inclined towards gender bias and the ones which mitigate it. Miss rates metric is effective in finding out potential bias in predictions. Our study utilizes miss rates metric along with a standard metric such as accuracy, precision or recall to evaluate possible gender bias effectively.},
  comment         = {Gender bias detection in ML algorithms

Bias in facial recognition},
  date            = {5-7 Dec. 2021},
  doi             = {10.1109/SSCI50451.2021.9660186},
  eventdate       = {5-7 Dec. 2021},
  eventtitleaddon = {Orlando, FL, USA},
  file            = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Evaluation_of_Gender_Bias_in_Facial_Recognition_with_Traditional_Machine_Learning_Algorithms.pdf:PDF},
  groups          = {Gender Bias},
  isbn            = {978-1-7281-9049-5},
  journal         = {2021 IEEE Symposium Series on Computational Intelligence (SSCI)},
  keywords        = {Training, Measurement, Machine learning algorithms, Image recognition, Face recognition, Static VAr compensators, Prediction algorithms, facial recognition, machine learning, gender, bias, fairness, race, equality, inclusivity, diversity},
  ranking         = {rank4},
}

@Article{Bhargava2019,
  author        = {Shruti Bhargava and David Forsyth},
  title         = {Exposing and Correcting the Gender Bias in Image Captioning Datasets and Models},
  year          = {2019},
  month         = dec,
  abstract      = {The task of image captioning implicitly involves gender identification. However, due to the gender bias in data, gender identification by an image captioning model suffers. Also, the gender-activity bias, owing to the word-by-word prediction, influences other words in the caption prediction, resulting in the well-known problem of label bias. In this work, we investigate gender bias in the COCO captioning dataset and show that it engenders not only from the statistical distribution of genders with contexts but also from the flawed annotation by the human annotators. We look at the issues created by this bias in the trained models. We propose a technique to get rid of the bias by splitting the task into 2 subtasks: gender-neutral image captioning and gender classification. By this decoupling, the gender-context influence can be eradicated. We train the gender-neutral image captioning model, which gives comparable results to a gendered model even when evaluating against a dataset that possesses a similar bias as the training data. Interestingly, the predictions by this model on images with no humans, are also visibly different from the one trained on gendered captions. We train gender classifiers using the available bounding box and mask-based annotations for the person in the image. This allows us to get rid of the context and focus on the person to predict the gender. By substituting the genders into the gender-neutral captions, we get the final gendered predictions. Our predictions achieve similar performance to a model trained with gender, and at the same time are devoid of gender bias. Finally, our main result is that on an anti-stereotypical dataset, our model outperforms a popular image captioning model which is trained with gender.},
  archiveprefix = {arXiv},
  comment       = {Gender bias in ML algorithms},
  eprint        = {1912.00578},
  file          = {:- Exposing and Correcting the Gender Bias in Image Captioning Datasets and Models.pdf:PDF},
  groups        = {Gender Bias},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
  ranking       = {rank4},
}

@Article{Dinan2020,
  author        = {Emily Dinan and Angela Fan and Ledell Wu and Jason Weston and Douwe Kiela and Adina Williams},
  title         = {Multi-Dimensional Gender Bias Classification},
  year          = {2020},
  month         = may,
  abstract      = {Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites. Distinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers. We show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models, detecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness.},
  archiveprefix = {arXiv},
  comment       = {Text gender bias


Framework that decomposes gender bias in text along several pragmatic and semantic dimensions},
  eprint        = {2005.00614},
  file          = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Multi Dimensional Gender Bias Classification.pdf:PDF},
  groups        = {Gender Bias},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
  ranking       = {rank5},
}

@InProceedings{Katsarou2022,
  author  = {Katsarou, Styliani and Rodriguez Gálvez, Borja and Shanahan, Jesse},
  title   = {Measuring Gender Bias in Contextualized Embeddings},
  year    = {2022},
  month   = {04},
  pages   = {3},
  comment = {Text gender bias


hard to understand

Gender bias in context},
  doi     = {10.3390/cmsf2022003003},
  file    = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/csmf-03-00003.pdf:PDF},
  groups  = {Gender Bias},
  ranking = {rank4},
}

@Article{Tannenbaum2019,
  author        = {Tannenbaum, Cara and Ellis, Robert P. and Eyssel, Friederike and Zou, James and Schiebinger, Londa},
  journal       = {Nature},
  title         = {Sex and gender analysis improves science and engineering},
  year          = {2019},
  number        = {7781},
  pages         = {137--146},
  volume        = {575},
  abstract      = {The goal of sex and gender analysis is to promote rigorous, reproducible and responsible science. Incorporating sex and gender analysis into experimental design has enabled advancements across many disciplines, such as improved treatment of heart disease and insights into the societal impact of algorithmic bias. Here we discuss the potential for sex and gender analysis to foster scientific discovery, improve experimental efficiency and enable social equality. We provide a roadmap for sex and gender analysis across scientific disciplines and call on researchers, funding agencies, peer-reviewed journals and universities to coordinate efforts to implement robust methods of sex and gender analysis.},
  bdsk-url-1    = {https://doi.org/10.1038/s41586-019-1657-6},
  comment       = {General Overview of Sex and gender analysis


Distinction between Gender and Sex.

Figure 2 interesting to cite (Sex).
Figure 3 interesting to cite (Gender).},
  date          = {2019/11/01},
  date-added    = {2022-04-30 14:32:01 +0200},
  date-modified = {2022-04-30 14:32:01 +0200},
  doi           = {10.1038/s41586-019-1657-6},
  file          = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Sex and gender analysis improves science and engineering.pdf:PDF},
  groups        = {Gender Bias},
  id            = {Tannenbaum2019},
  isbn          = {1476-4687},
  ranking       = {rank4},
  url           = {https://doi.org/10.1038/s41586-019-1657-6},
}

@Article{Wagner2016,
  author    = {Wagner, Claudia and Graells-Garrido, Eduardo and Garcia, David and Menczer, Filippo},
  journal   = {EPJ Data Science},
  title     = {Women through the glass ceiling: gender asymmetries in Wikipedia},
  year      = {2016},
  pages     = {1--24},
  volume    = {5},
  comment   = {Text classification


Gender bias on Wikipedia (text)},
  file      = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/1601.04890.pdf:PDF},
  groups    = {Text Classification},
  publisher = {Springer},
  ranking   = {rank5},
  url       = {https://link.springer.com/content/pdf/10.1140/epjds/s13688-016-0066-4.pdf},
}

@InProceedings{Levi2015,
  author    = {Levi, Gil and Hassncer, Tal},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title     = {Age and gender classification using convolutional neural networks},
  year      = {2015},
  pages     = {34-42},
  comment   = {Image classification

Gender classification in images with CNN},
  doi       = {10.1109/CVPRW.2015.7301352},
  file      = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Age_and_gender_classification_using_convolutional_neural_networks.pdf:PDF},
  groups    = {Image Classification},
  ranking   = {rank4},
}

@Article{Sun2019,
  author  = {Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and ElSherief, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang},
  journal = {arXiv preprint arXiv:1906.08976},
  title   = {Mitigating gender bias in natural language processing: Literature review},
  year    = {2019},
  comment = {ML algorithm bias detection


Literature review for mitigatin bias in NLP},
  file    = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/1906.08976.pdf:PDF},
  groups  = {Gender Bias},
  ranking = {rank4},
}

@Article{Rangel2018,
  author  = {Rangel, Francisco and Rosso, Paolo and Montes-y-G{\'o}mez, Manuel and Potthast, Martin and Stein, Benno},
  journal = {Working Notes Papers of the CLEF},
  title   = {Overview of the 6th author profiling task at pan 2018: multimodal gender identification in twitter},
  year    = {2018},
  pages   = {1--38},
  file    = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/rangel_2018.pdf:PDF},
  groups  = {Text Classification},
}

@Article{Cislak2018,
  author        = {Cislak, Aleksandra and Formanowicz, Magdalena and Saguy, Tamar},
  journal       = {Scientometrics},
  title         = {Bias against research on gender bias},
  year          = {2018},
  number        = {1},
  pages         = {189--200},
  volume        = {115},
  abstract      = {The bias against women in academia is a documented phenomenon that has had detrimental consequences, not only for women, but also for the quality of science. First, gender bias in academia affects female scientists, resulting in their underrepresentation in academic institutions, particularly in higher ranks. The second type of gender bias in science relates to some findings applying only to male participants, which produces biased knowledge. Here, we identify a third potentially powerful source of gender bias in academia: the bias against research on gender bias. In a bibliometric investigation covering a broad range of social sciences, we analyzed published articles on gender bias and race bias and established that articles on gender bias are funded less often and published in journals with a lower Impact Factor than articles on comparable instances of social discrimination. This result suggests the possibility of an underappreciation of the phenomenon of gender bias and related research within the academic community. Addressing this meta-bias is crucial for the further examination of gender inequality, which severely affects many women across the world.},
  bdsk-url-1    = {https://doi.org/10.1007/s11192-018-2667-0},
  comment       = {Bias against research on gender bias},
  date          = {2018/04/01},
  date-added    = {2022-05-02 19:23:33 +0200},
  date-modified = {2022-05-02 19:23:33 +0200},
  doi           = {10.1007/s11192-018-2667-0},
  file          = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Cislak2018_Article_BiasAgainstResearchOnGenderBia.pdf:PDF},
  groups        = {Gender Bias},
  id            = {Cislak2018},
  isbn          = {1588-2861},
  ranking       = {rank4},
  url           = {https://doi.org/10.1007/s11192-018-2667-0},
}

@Article{Geisberger2017,
  author  = {Geisberger, Tamara and Glaser, Thomas},
  journal = {Analysen zum Einfluss unterschiedlicher Faktoren auf den geschlechtsspezifischen Lohnunterschied. Statistische Nachrichten (6)},
  title   = {Gender Pay Gap},
  year    = {2017},
  pages   = {460--471},
  file    = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Gender Pay_Gap_Geisberger und Glaser_2017.pdf:PDF},
  groups  = {Gender Bias},
}

@Article{Chhabra2016,
  author    = {Chhabra, Meghna and Karmarkar, Yamini},
  journal   = {ZENITH International Journal of Multidisciplinary Research},
  title     = {Gender gap in entrepreneurship-a study of small and micro enterprises},
  year      = {2016},
  number    = {8},
  pages     = {82--99},
  volume    = {6},
  file      = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/GENDERGAPINENTREPRENEURSHIPASTUDYOFSMALLANDMICROENTERPRISES.pdf:PDF},
  groups    = {Gender Bias},
  publisher = {ZENITH International Research \& Academic Foundation (ZIRAF) India},
}

@InProceedings{Crotti2021,
  author       = {Crotti, Robert and Pal, Kusum Kali and Ratcheva, Vesselina and Zahidi, Saadia},
  title        = {The global gender gap report 2021},
  year         = {2021},
  organization = {World Economic Forum},
  file         = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/WEF_GGGR_2021.pdf:PDF},
  groups       = {Gender Bias},
}

@InCollection{Dwivedi2019,
  author    = {Dwivedi, Neelam and Singh, Dushyant Kumar},
  booktitle = {Harmony Search and Nature Inspired Optimization Algorithms},
  publisher = {Springer},
  title     = {Review of deep learning techniques for gender classification in images},
  year      = {2019},
  pages     = {1089--1099},
  file      = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Review of Deep Learning Techniques for Gender Classification in Images.pdf:PDF},
  groups    = {Image Classification},
}

@InProceedings{Kumar2019,
  author       = {Kumar, Sandeep and Singh, Sukhwinder and Kumar, Jagdish},
  booktitle    = {2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
  title        = {Gender classification using machine learning with multi-feature method},
  year         = {2019},
  organization = {IEEE},
  pages        = {0648--0653},
  file         = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Gender_Classification_Using_Machine_Learning_with_Multi-Feature_Method.pdf:PDF},
  groups       = {Image Classification},
}

@InProceedings{Jiang2020,
  author    = {Jiang, Zebin},
  booktitle = {2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
  title     = {Face Gender Classification Based on Convolutional Neural Networks},
  year      = {2020},
  pages     = {120-123},
  doi       = {10.1109/CIBDA50819.2020.00035},
  file      = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/Face_Gender_Classification_Based_on_Convolutional_Neural_Networks.pdf:PDF},
  groups    = {Image Classification},
}

@InProceedings{Jia2015,
  author    = {Jia, Sen and Lansdall-Welfare, Thomas and Cristianini, Nello},
  booktitle = {Proceedings of the 24th International Conference on World Wide Web},
  title     = {Measuring gender bias in news images},
  year      = {2015},
  pages     = {893--898},
  file      = {:/Users/finjukka/Library/CloudStorage/OneDrive-Personal/_data_science/WIA/Literatur Review/literature/10.1.1.994.5778.pdf:PDF},
  groups    = {Gender Bias},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Text Classification\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Image Classification\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Gender Bias\;0\;1\;0x8a8a8aff\;\;\;;
}
